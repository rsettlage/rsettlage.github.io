[
  {
    "path": "deeplearning/",
    "title": "DeepLearning Inspiration",
    "description": "Details of why DeepLearning for me.",
    "author": [
      {
        "name": "Robert Settlage",
        "url": {}
      }
    ],
    "date": "2022-05-31",
    "categories": [
      "DeepLearning",
      "Perceptron"
    ],
    "contents": "\n\nContents\nPerceptron\nInputs\nWeights\nZ\nActivation function\nOutputs\n\nMulticlass perceptrons\nMultilayer Perceptrons\n\nI like computer simulations. That is like dumb science though. What\nif the computer could actually learn and make decisions. Woah. Perhaps\nthat is a little too Terminator for where we are at currently, but, how\nfar away is that? Probably not terribly far off. As with the MCMC\ncollection, I am going to start at the start of my understanding and see\nwhere I get. In the realm of Deep Learning, I particularly like the\nadversarial networks and reinforcement learning so will likely spend\nmore time on those.\nHere, I am going to start with what I see as the motivation for\nDeepLearning, biology and the perceptron. A particularly good coffee\ntable resource is:\n\n\n\nGoodFellow\n@book{Goodfellow-et-al-2016,\n    title={Deep Learning},\n    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n    publisher={MIT Press},\n    note={\\url{http://www.deeplearningbook.org}},\n    year={2016}\n}\nMuch of AI/Machine Learning/Deep Learning history is steeped in\nreplicating nature, specifically the functions of the brain. Drilling\ndown into the brain, we get to neurons. Neurons take input from\ndendrites, pass these as signals through the axon, which are then\ntransmitted through synapses to the dendrites of another neuron.\nModelling the functional behavior of neurons is a feat. Enter the\nperceptron.\nWiki neuron\nPerceptron\nThe Wiki for\nPerceptron’s is pretty complete and I will just quote it:\n\nIn machine learning, the perceptron is an algorithm for supervised\nlearning of binary classifiers. A binary classifier is a function which\ncan decide whether or not an input, represented by a vector of numbers,\nbelongs to some specific class. It is a type of linear classifier,\ni.e. a classification algorithm that makes its predictions based on a\nlinear predictor function combining a set of weights with the feature\nvector.\n\nSo, the perceptron is essentially an over simplified model of our\nunderstanding of neurons. The perceptron is an algorithm that can be\ntuned to perform binary classification tasks. Since we are simplifying,\nlet’s draw the neuron using input/output labels as our new\nperceptron.\n\n\n\nFigure 1: Simple perceptron.\n\n\n\nAbove, in our over simplified neuron, we see the components of our\nperceptron are inputs, x’s, importance weights, w’s, an activation\nfunction, f(z), and output, y. We will talk through these parts and then\nimplement this in a simple learning model.\nInputs\nIn our perceptron understanding of neurons, we accept inputs as the\nstart to our algorithm. The inputs can be any type of data, for\ninstance, continuous values such as height, weight, or width, or even\ndiscrete values such as counts or class memberships such as word labels\nin text analytics. When we think of these inputs in our neuronal\nunderstanding, they will often be combined with a bias as in the\naugmented image below. In the neuron, the bias is considered a threshold\nvalue below which the neuron will not “fire” or activate. In our\nperceptron, this is limiting to the case of binary classification,\nuseful, but not complete in that we can use the perceptron for tasks\nother than binary classification as we will see below.\n\n\n\nFigure 2: Perceptron with bias drawn with draw.io.\n\n\n\nWeights\nThese are the things we need, the parameters of the model. In a\nlinear models class, these would be our \\(\\beta\\)’s. The weights are numerical values\nthat indicate the importance of the specific input dimension, ie \\(x_i\\). The perceptron learning algorithm\nwill learn these through iteration, likely using some permutation of\ngradient descent. As mentioned above, we are adding a bias to the\nalgorithm. This is functionally equivalent to an intercept in a linear\nmodel. I have labeled it as “b” and given it a weight, \\(w_0\\), it is convenient to assume b=1 and\nadd this as a dimension to our data.\nZ\nWe need a function to combine the data with the appropriate weights.\nThis is generally given the sybmol “z” or in some figures the symbol for\nsum: \\(\\sum\\). Setting the bias to 1\nand learning the associate weight leaves us with: \\[\n\\begin{equation}\n\\tag{1}\n\\textbf{z} = \\sum_i w_i \\ast x_i = \\textbf{w} \\cdot \\textbf{x}\n\\end{equation}\n\\]\nActivation function\nThe activation function, \\(f(z)\\)\ncan take on many forms. In it’s simplest form, it is simply the\nidentity, ie \\(f(z)=z\\). This\neffectively makes the perceptron a linear model. Turning to\nclassification, the activation function becomes the Heavyside step\nfunction, or a mapping \\(z\\) to\n(0,1).\n\\[\n\\begin{equation}\n\\tag{2}\nf(z) =\n\\begin{cases}\n1 \\text{ if } z = \\textbf{w} \\cdot \\textbf{x} > 0 \\\\\n0 \\text{ otherwise}\n\\end{cases}\n\\end{equation}\n\\]\nActivation functions are a varied and active area of research. Common\nactivations functions in neural networks include ReLU, sigmoid, and tanh\namong others. Choice of activation function depends highly on the goals\nof the algorithm and will be discussed in its own future post.\nOutputs\nThe output of the perceptron is our decision. If using the step\nfunction given in (2), the decision may be something as simple as is the\nimage a cat (+1) or not a cat (0). Turning to linear regression, the\noutput is our z’s, or y’s if that is more familiar.\n\nMulticlass perceptrons\nMultilayer Perceptrons\n\n\n\n",
    "preview": "deeplearning/images/perceptron.drawio.png",
    "last_modified": "2022-06-01T14:04:53-04:00",
    "input_file": {},
    "preview_width": 327,
    "preview_height": 251
  }
]
