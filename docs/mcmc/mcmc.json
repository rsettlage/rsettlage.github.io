[
  {
    "path": "mcmc/",
    "title": "tl;dr Monte Carlo",
    "description": "Basic Monte-Carlo description and theory.",
    "author": [
      {
        "name": "Robert Settlage",
        "url": {}
      }
    ],
    "date": "2022-06-01",
    "categories": [
      "MCMC"
    ],
    "contents": "\n\nContents\nMonte Carlo\nGeneral idea\n\nExample 1\nExample 2\nBias, variance and\nconvergence\nBias\n\n\nMonte-Carlo summary. Much of this is following notes from Scotland\nLemon’s MCMC class in Spring 2017 at Virginia Tech.\nMonte Carlo\nClass or set of algorithms to produce a random and approximate\nsolution to a problem in a \\(\\textbf{fixed}\\) amount of time. As opposed\nto Las Vegas algorithms that produce a solution of fixed tolerance in an\nunknown amount of time.\nMonte Carlo methods rely on sampling \\(iid\\) from the distribution to then compute\nthe estimate. The class and what I am interested in more are random\nwalks where the next sample is dependent on the current sample. This is\nmore aptly described as Markov Chain Monte Carlo.\nGeneral idea\nif \\(x\\sim p(x)\\), where we can\nsample from \\(p(x)\\), but we want \\(g(x)\\), then\n\\[\n\\begin{eqnarray}\n\\tag{1}\nE[g(x)] &=& \\int_{\\Omega_x}g(x)p(x)dx \\\\\n  &\\approx& \\frac{\\sum g(x)}{N} \\text{where }x_i\\sim p(x)\n\\end{eqnarray}\n\\]\nExample 1\nWe want \\(E[u]\\) where \\(u\\sim unif[0,1]\\). The pdf of x is\n\\[\n\\begin{equation}\n\\tag{2}\nx \\sim\n\\begin{cases}\n1 \\text{ if } 0 \\le u \\le 1 \\\\\n0 \\text{ otherwise}\n\\end{cases}\n\\end{equation}\n\\] So we could do this analytically as:\n\\[\n\\begin{equation}\n\\tag{3}\nE[u] = \\int_0^1 u\\;p(u)\\;du = \\frac{u^3}{2} \\mid _0^1 =\n\\frac{1}{2}\\text{ similarly, var}(u)=\\frac{1}{12}\n\\end{equation}\n\\] Alternatively, using Monte-Carlo approximation, we would\nfollow:\ninit \\(u_0 = runif(1)\\)\nfor \\(i=1:N\\)\\(u_i = runif(1)\\)\nend\nNow:\n\\[\n\\begin{eqnarray}\n\\tag{4}\nmean(u) &=& \\frac{\\sum_{i=1}^N u_i}{N} \\approx \\int_0^1\nu\\;p(u)\\;du \\\\\n\\tag{5}\nVar(u) &=& \\frac{\\sum_{i=1}^N (u_i-E[u])^2}{N} \\approx \\int_0^1\n(u_i-E[u])^2\\;p(u)\\;du\n\\end{eqnarray}\n\\] Quick try with N going from 100 to 10,000:\n\n\nset.seed(12475)\ndraws <- runif(10000)\nresults <- data.frame(means=rbind(mean(draws[1:100]),mean(draws[1:1000]),mean(draws)),\n                      vars=rbind(var(draws[1:1000]),var(draws[1:100]),var(draws)),\n                      row.names = c(\"100\",\"1,000\",\"1,0000\"))\nkable(results,digits = 4)\n\n\n\n\n\nmeans\n\n\nvars\n\n\n100\n\n\n0.5131\n\n\n0.0848\n\n\n1,000\n\n\n0.4950\n\n\n0.0816\n\n\n1,0000\n\n\n0.5009\n\n\n0.0856\n\n\nLooks like the mean is approaching the desired value of 0.5.\nExample 2\n\\(x\\sim N(\\mu=10,\n\\sigma^2=1)\\)\nWant \\[\n\\begin{equation}\n\\tag{6}\nE[x^4] = \\int_{-\\infty}^{\\infty} x^4\\;p(x)\\;dx =\n\\int_{-\\infty}^{\\infty}x^4\\;\\frac{e^{-\\tfrac{(x-E[x])^2}{2}}}{\\sqrt{2\\pi}}\\;dx\n= ??\n\\end{equation}\n\\] Could do this integral using some tricks, but instead, perhaps\nuse Monte Carlo method:\ninit \\(x_0 = rnorm(10,1)\\)\nfor \\(i=1:N\\)\\(x_i = rnorm(10,1)\\)\nend\nNow:\n\\[\n\\begin{equation}\n\\tag{7}\nE[X^4] \\approx \\frac{\\sum_{x_i\\sim N(10,1)}x_i^4}{N}\n\\end{equation}\n\\]\n\n\nset.seed(12475)\ndraws <- rnorm(10000,10,1)^4\nresults <- data.frame(means=rbind(mean(draws[1:100]),mean(draws[1:1000]),mean(draws)),\n                      row.names = c(\"100\",\"1,000\",\"1,0000\"))\nkable(results,digits = 4)\n\n\n\n\n\nmeans\n\n\n100\n\n\n10258.86\n\n\n1,000\n\n\n10608.94\n\n\n1,0000\n\n\n10615.04\n\n\nIt appears to be converging. The variance seems to converge as well,\nbut the scale makes it less informative. Show sd? eh.\nBias, variance and\nconvergence\nCouple things to clean up to close, is the estimate biased and how\ndoes it converge? For this, let’s assume we are looking at\n\\(u\\stackrel{iid}{\\sim}unif(0,1)\\text{;\ni=1..N}\\)\n\\(E[u]\\approx\n\\frac{\\sum_{u_i\\stackrel{iid}{\\sim}unif(0,1)}u_i}{N}=\\hat{M}\\)\nand similar for variance.\nBias\nIs the estimate biased?\n\\[\n\\begin{eqnarray}\n\\tag{8}\nE[\\hat{M}] &=& E\\left[\\frac{\\sum_{i=1}^Nu_i}{N}\\right] \\\\\n  &=& \\frac{\\sum_{i=1}^N E[u_i]}{N} \\\\\n  &=& \\frac{\\sum_{i=1}^N E[u]}{N} \\text{ b/c iid}\\\\\n  &=& E[u] \\\\\n  &=& M\n\\end{eqnarray}\n\\]\n\\[\n\\begin{eqnarray}\n\\tag{9}\nVar[\\hat{M}] &=& Var\\left[\\frac{\\sum_{i=1}^Nu_i}{N}\\right] \\\\\n  &=& \\frac{\\sum_{i=1}^N Var[u_i]}{N^2} \\\\\n  &=& \\frac{\\sum_{i=1}^N Var[u]}{N^2} \\text{ b/c iid}\\\\\n  &=& \\frac{Var[u]}{N} \\\\\n  &\\xrightarrow[]{N\\rightarrow \\infty}& 0\n\\end{eqnarray}\n\\]\nSo our estimate is unbiased with zero variance. Can we say anything\nabout convergence in probability?\n\\[\n\\begin{eqnarray}\n\\tag{10}\nPr(\\mid \\hat{M}-M \\mid \\lt \\epsilon) &=& Pr\\left(\\mid \\hat{M}-M\n\\mid \\lt \\delta \\cdot \\sqrt{V/N}\\right) \\text{ where V=Var(M)} \\\\\n  &=& Pr\\left(\\frac{\\mid \\hat{M}-M \\mid}{\\sqrt{V/N}} \\lt\n\\delta\\right) \\text{note this is }N(0,1)\\lt\\delta\\\\\n  &\\approx& Pr\\left(\\frac{\\mid \\hat{M}-M \\mid}{\\sqrt{\\hat{V}/N}}\n\\lt \\delta\\right) \\text{note }\\hat{V}\\text{ is sample estimate of\nvariance, now this is }t(0,1)\\lt\\delta\n\\end{eqnarray}\n\\]\nAnd by the CLT, we can make probabilistic statements. We also note\nour probabilistic error bound only depends on N through \\(\\sqrt{\\frac{V}{N}}\\approx\\sqrt{\\frac{\\hat{V}}{N}}\\)\nso that for any dimensional problem, the error descreases at the rate of\n\\(N^{-\\tfrac{1}{2}}\\) such that we are\n\\(O(N^{-\\tfrac{1}{2}})\\)\nconvergence.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-11T11:45:22-04:00",
    "input_file": "Monte-Carlo-tldr.knit.md"
  }
]
