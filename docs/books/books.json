[
  {
    "path": "books/2022-07-03-hobbs-hooton-ch2/",
    "title": "Hobbs-Hooten Chapter 2",
    "description": "A quick summary of Hobbs and Hooten's Bayesian Models, Chapter 2 -- Deterministic Models.",
    "author": [
      {
        "name": "Robert Settlage",
        "url": {}
      }
    ],
    "date": "2022-07-03",
    "categories": [
      "Deterministic models"
    ],
    "contents": "\n\nContents\nTheoretical models\nEmpirical models\nSimulation models\nDeterministic\nfunctions highlighted\nFunctions of additive\neffects\nAsymptotic functions\nThresholds\n\n\nDistillation of Chapter 2 from Hobbs and Hooten. (While a\ndistillation, some sentences are so concise as to warrant taking\nverbatim. Please excuse the lack of quotes as an oversight.) The authors\ngive our goal as “combining data with models to provide insight about\necology by answering a fundamental question: what is the probability I\nthat I will observe the data if the model faithfully represents the\nprocesses that give rise to the data?” The starting point in the book is\ngiven as considering the ecological process by writing a deterministic\nequation(s). This is given as the “Design” phase where we combine\nexisting theory with our objectives and intuition to create a model we\ncan combine with existing or future data. The authors start by\nrecognizing three styles of modeling traditions in ecology they call\ntheoretical, empirical, and simulation.\nTheoretical models\nThese ideas come from mathematical analysis of systems of\ndeterministic, nonlinearl differential, and difference equations. This\nis a no data required approach through which insight is derived from the\nmathematical properties of equilibrium points and local stability\nconditions etc. They need to be simplistic due to the intractability of\nnonlinear systems. These models have as a strength that the parameters\nare defined in biological terms and they symbolically represent how a\nprocess works. The derived hypothesis are often tested through empirical\nmodels.\nEmpirical models\nThese models describe relationships in data. They depend on a\ndeterministic model, but are often linear models. The parameters are\nlacking in portability and biological interpretation. The strength of\nthese models comes from the rigorous treatment of uncertainty even if\nthey lack complexity.\nSimulation models\nSimulation models allow exploration and modeling of complex systems\ninvolving many parameters and interactions. Through simulation, the\necologist can explore the state space across many spatial scales and\nlevels of organization.\nThe authors note that each of the modeling modes have strengths and\nwhat we need to do is merge the styles. This book uses the Baysian\nHierarchical framework to do just that.\nDeterministic functions\nhighlighted\nA deterministic model is a function, \\(g()\\), that returns and output \\(\\mu\\) representing the true state \\(z\\) based on parameters (\\(\\theta_p\\)) and observations (\\(\\mathbf{x}\\)):\n\\[\n\\begin{equation}\n\\tag{1}\n\\mu = g(\\theta_p,\\mathbf{x})\n\\end{equation}\n\\]\nCare will be taken to make sure the range and domains of outpus and\ninputs (respectively) are maintained.\nFunctions of additive\neffects\nThe simple linear model:\n\\[\n\\begin{equation}\n\\tag{2}\ng(\\underline{\\smash{\\beta}},\\mathbf{x}) = \\beta_0 + \\beta_1 x_1 + \\dots\n+ \\beta_n x_n\n\\end{equation}\n\\]\nNoting that interactions can be expressed through product terms and\ncurvilinear terms through powers of \\(x\\). Note that the range is all real\nnumbers.\nExponential\n\\[\n\\begin{equation}\n\\tag{2}\ng(\\underline{\\smash{\\beta}},\\mathbf{x}) = e^{\\beta_0 + \\beta_1 x_1 +\n\\dots + \\beta_n x_n}\n\\end{equation}\n\\] This is useful in mapping the range to (0,\\(\\infty\\)).\nInverse logit\n\\[\n\\begin{equation}\n\\tag{3}\ng(\\underline{\\smash{\\beta}},\\mathbf{x}) = \\frac{e^{\\beta_0 + \\beta_1 x_1\n+ \\dots + \\beta_n x_n}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n\nx_n}}\n\\end{equation}\n\\] This is useful in modeling proportions by mapping the range to\n(0,1). This function is the inverse of the logit function:\n\\[\n\\begin{equation}\n\\tag{4}\nlogit(p) = ln\\left ( \\frac{p}{1-p}\\right )\n\\end{equation}\n\\] ### Power functions\n\\[\n\\begin{eqnarray}\n\\tag{5}\ng(\\underline{\\smash{\\beta},x}) &=& \\beta_0x^{\\beta_1} \\text{ or\ntransformed: }\\\\\n\\tag{6}\nln(g(\\underline{\\smash{\\beta},x})) = ln(\\mu ) &=& ln(\\beta_0) +\n\\beta_1 ln(x)\n\\end{eqnarray}\n\\]\nadditional \\(\\beta\\)’s can be\nincluded as needed.\nAsymptotic functions\nThe Mechaelis-Menten equation:\n\\[\n\\begin{equation}\n\\tag{7}\ng(\\nu_{max},k,x) = \\frac{\\nu_{max}x}{k+x}\n\\end{equation}\n\\] Raising x and k to the power of q gives the Hill function. If\n\\(\\nu_{max}=\\frac{1}{\\gamma}\\) and\n\\(k=\\frac{1}{\\alpha\\gamma}\\), then you\narrive at the Type II functional response.\nOthers include:\nMonomolecular function:\n\\[\n\\begin{equation}\n\\tag{8}\ng(\\alpha, \\gamma, x) = \\alpha(1-e^{-\\gamma x})\n\\end{equation}\n\\]\nGompertz function:\n\\[\n\\begin{equation}\n\\tag{9}\ng(\\alpha, \\gamma, x) = e^{-\\alpha e^{-\\gamma x}}\n\\end{equation}\n\\]\nThresholds\nMany situations arise where there is an inflection point, \\(\\tau\\), where the dynamics of the model\nswitch between behaviors:\n\\[\n\\begin{equation}\n\\tag{8}\n\\mu = \\begin{cases}\ng_1(\\mathbf{\\theta}_1,x) \\text{ if } x \\lt \\tau \\\\\ng_2(\\mathbf{\\theta}_2,x) \\text{ otherwise } x \\ge \\tau\n\\end{cases}\n\\end{equation}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-03T10:03:30-04:00",
    "input_file": {}
  },
  {
    "path": "books/2022-07-03-hobbs-hooton-ch3/",
    "title": "Hobbs-Hooten Chapter 3",
    "description": "A quick summary of Hobbs and Hooten's Bayesian Models, Chapter 3 -- Principles of Probability.",
    "author": [
      {
        "name": "Robert Settlage",
        "url": {}
      }
    ],
    "date": "2022-07-03",
    "categories": [
      "Deterministic models"
    ],
    "contents": "\n\nContents\nProcess variance\nObservation variance\nIndividual variation\nModel selection\nuncertainty\nRules of probability\nSample space, outcomes,\nand events\nConditional,\nindependent, and disjoint probabilities\nFactoring joint\nprobabilities\n\n\nThe models we create are deliberate approximations of truth.\nDeliberate in that we intentionally (or not) simplify the model to\nsomething understandable and tractable with the data at hand. Statistics\nhelp us gain insight into the approximations by understanding and\nquantifying our uncertainty. The main sources of variation include:\nProcess variance\nProcess variance includes all the uncertainty associated with our\nimperfect model specification. Good models have low process variance\nbecause they capture the majority of the variation in the state they\npredict. The only way to reduce process variance is to improve our\nmodel, ie collecting more of the same data, improving our\ninstrumentation, etc will not help with a poorly specified model. This\nhighlights the need to separate process variance from other sources of\nvariance such as observation variance.\nObservation variance\nObservation variance is the variance associated with an actual\nobservation of the state of the system. The two causes of observation\nvariance are sampling from a larger population and potential biases in\nhow we collect the observations. Sampling variance is reduced through\nmore sampling while corrections in sampling bias becomes more certain\nwith additional samples.\nIndividual variation\nFor processes where we are interested in individuals, differences in\nindividuals themselves give rise to uncertainty. Spatial location can\nalso be thought of the same way as individual variations.\nModel selection uncertainty\nThis uncertainty arises from our choice of a specific model. In many\ncases, we may not care about the uncertainty associated with a specific\nmodel, in others, we may want to quantify the uncertainties associated\nwith different choices.\nRules of probability\nFinally. Something both concrete and squishy. ;)\nWe are seeking to learn about unobserved quantities from data,\nobserved quantities. Bayesians treat all unobserved quantities as random\nvariables. Random variables, are, well random and can take on a range of\nvalues due to chance. Chance, being governed by the rules of\nprobability, and taking values according to some probability\ndistribution.\nSample space, outcomes, and\nevents\noutcome - a possible result of an experiment\nevent - a set of possible outcomes of an experiment\nsample space – set of all possible outcomes of an experiment\nConsider rolling a 6-sided die. The sample space contains the numbers\n1-6 as outcomes. An event could include evens (2,4,6) vs odds\n(1,3,5).\nConditional,\nindependent, and disjoint probabilities\nGiven 2 events, we can talk about conditional, independent, and\ndisjoint probabilities.\nConditional – if the probability of occurrence of event B is\ndependent on event A, this means that knowledge of occurrence of event B\nchanges the probability of event A. For instance, suppose we are pulling\nballs out of a bag where 2 of the 5 balls in the bag are blue, the rest\nare red. The probability of choosing a blue ball on the first draw is\n2/5. After drawing a blue ball, now drawing a second ball, the\nprobability of pulling a second blue ball is 1/4. This is written\nas:\n\\[\n\\tag{1}\nP(A \\mid B) = \\frac{P(A,B)}{P(B)}\n\\]\nindependent\nIf occurrence of one event does not change the probability of the\nother, the events are said to be independent. For example, flipping a\ncoin twice. The result of heads on the first flip does not influence the\nsecond flip. This is written as:\n\\[\n\\begin{eqnarray}\n\\tag{2}\nP(A \\text{ and } B) = P(A,B) = P(A) \\ast P(B) \\\\\n\\tag{3}\nP(A|B) = P(A)\n\\end{eqnarray}\n\\]\ndisjoint\nIf two events are disjoint, the probability of both events occurring\nis 0. Written as:\n\\[\n\\tag{4}\nP(B\\mid A) = \\frac{P(A,B)}{P(A)} = 0\n\\]\nNOTE, this is different than independent. Knowing that event A\noccurred in a pair of disjoint events leads to the knowledge that event\nB has NOT occurred. For a pair of independent events, this statement can\nnot be made.\nmisc helpful equations\nUnion or inclusive or \\[\n\\tag{5}\nP(A \\cup B) = P(A) + P(B) - P(A,B)\n\\]\nSample space (S) is partitioned into n disjoint sets (\\(B_n\\)) and we are interested in event A\nthat may overlap 1 or more events \\(B_n\\): \\[\n\\tag{6}\nP(A) = \\sum_n P(A|B_n)P(B_n)\n\\]\nOr, as n approaches infinity: \\[\n\\tag{7}\nP(A) = \\int [A|B][B] dB\n\\]\nFactoring joint\nprobabilities\nIt will be helpful to simplify our thoughts later if we can factor\njoint probabilities. To do so, starting from a rearrangement of (1)\n\\[\n\\tag{8}\nP(A,B) = P(A\\mid B)P(B)\n\\]\nWe can then write:\n\\[\n\\tag{9}\nP(a_1,a_2, \\dots a_n\\mid p_1,p_2,\\dots p_n ) = \\prod_{i=1}^n P(a_i\\mid\n\\{p_i\\})\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-21T22:00:31-04:00",
    "input_file": {}
  },
  {
    "path": "books/2022-07-02-hobbs-hooten-ch1/",
    "title": "Hobbs-Hooten Chapter 1",
    "description": "A quick summary of Hobbs and Hooten's Bayesian Models, Chapter 1 -- Preview.",
    "author": [
      {
        "name": "Robert Settlage",
        "url": {}
      }
    ],
    "date": "2022-07-02",
    "categories": [
      "Bayesian modeling",
      "Bayesian networks"
    ],
    "contents": "\n\nContents\nChapter 1\nProcess models\nSampling models\nObservation models\nParameter models\nFull model\n\nExample\nWildebeast population as a function of rainfall\nProcess model\nSampling model\nObservation model\nFull model\n\n\nI absolutely love this book. It is approachable and thorough. I am in\nmy second work through of the book and will just give the most brief\nnotes here. Besides the parallels between Picaso and Box, my favorite\nline in the book is “your science will have impact to the extent that\nyou are able to ask important questions and provide compelling answers\nto them”. I have always thought you should ask first the question you\nwant answered, then think about how you should go about gathering data\nto answer it. That hones the study to something achievable. In my write\nup, there are definitely phrases stolen, so consider this entire write\nup referenced.\nHobbs Hooten@book{10.2307/j.ctt1dr36kz,\nURL = {http://www.jstor.org/stable/j.ctt1dr36kz},\nabstract = {This textbook provides a comprehensive and accessible\nintroduction to the latest Bayesian methods-in language ecologists can\nunderstand.},\nauthor = {N. Thompson Hobbs and Mevin B. Hooten},\nedition = {STU - Student edition},\npublisher = {Princeton University Press},\ntitle = {Bayesian Models: A Statistical Primer for Ecologists},\nurldate = {2022-07-02},\nyear = {2015}\n}\nChapter 1\nThis is really setting the stage for the whole book. The main idea is\nthat an analysis of data tracks uncertainty through mathmatical modeling\nof the phenomena being observed to gain insight into the system. They\nbreak the process 4 main steps (stated in preface):\nDesign\nHere existing theory, scientific objectives and intuition combine to\nhelp write a deterministic model of the process\nModel Specification\nDiagram the relationship(s) between observed and unobserved quantities,\nuse these relationships to write out the posterior using general\nprobability notation, and finally choose the appropriate probability\ndistributions for the quantities\nModel implementation\nWrite full-conditional distribusions, write MCMC sampling algorithm,\ntest MCMC on simulated data, run MCMC algorithm on real data\nModel evaluation and inference\nPosterior predictive checks include probablistic inference from a single\nmodel and/or model selection and model averaging.\nHobbs and Hooten sketch out the framework for modelling as creating a\nseries of submodels that are chained to gether, Bayesian style. These\nmodels include: process models, sampling models, observational models,\nand parameter models.\nProcess models\nProcess models are mathematical statemensts that depict a process and\nour uncertainty about that process. Here we start by thinking about a\ntrue state (\\(z\\)) and are looking to\nunderstand influencers of the state, ie the things that make it change.\nTo the modeler, this will be a combination of a deterministic statement\nwith realizations of uncertainty. The deterministic part relates what we\nknow or can measure to our understanding of the system. We add\nuncertainty because we know we have omitted in our simplifications.\n\\[\n\\begin{equation}\n\\tag{1}\n\\left [ z \\mid g(\\theta_p, \\mathbf{x}), \\sigma_p^2 \\right ]\n\\end{equation}\n\\]\nSampling models\nFirst, we realize that in most studies, we can not observe all\ninstances of the true state of the system so we need to take samples\nfrom the system. The authors use \\(u_i\\) where \\(i =\n1 \\dots n\\) denote the observations taken.\n\\[\n\\begin{equation}\n\\tag{2}\n\\left [ u_i \\mid z, \\sigma_s^2 \\right ]\n\\end{equation}\n\\]\nObservation models\nThere is often a mismatch between what we observe and the true state,\nkinda a bias. The authors call this an observation model, I would call\nit a measurement model. Note that if we can observe perfectly samples\nfrom the true state, \\(y_i = u_i\\), ie\nwe do not need an observational model and our sampling model (and\nassociated sampling uncertainty) are enough to describe our\nobservations.\nParameter models\nWe have include parameters in our models that require some sort of\nunderstanding. We represent the parameters using probability\ndistributions. Ideally, we would have some prior knowledge of these\nparameters.\n\\[\n\\begin{equation}\n\\tag{3}\n\\left [ y_i \\mid d(\\theta_o, u_i), \\sigma_o^2 \\right ]\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\tag{4}\n\\left [ \\theta_p \\right ] \\left [ \\theta_o \\right ] \\left [ \\sigma_p^2\n\\right ] \\left [ \\sigma_s^2 \\right ] \\left [ \\sigma_o^2 \\right ]\n\\end{equation}\n\\]\nFull model\nPutting it all together, we can write down the posterior as:\n\\[\n\\begin{equation}\n\\tag{5}\n\\left [ {\\underbrace { z, \\theta_p, \\theta_o, \\sigma_p^2, \\sigma_s^2,\n\\sigma_o^2, u_i }_{\\text{unobserved}}} \\mid {\\underbrace { y_i\n}_{\\text{observed}}} \\right ] \\propto \\left [ {\\underbrace { y_i \\mid\nd(\\theta_o, u_i), \\sigma_o^2}_{\\text{observation model}}} \\right ] \\left\n[ {\\underbrace { u_i \\mid z, \\sigma_s^2}_{\\text{sampling model}}} \\right\n] \\ast  \\\\ \\left [ {\\underbrace { z \\mid g(\\theta_p, \\mathbf{x}),\n\\sigma_p^2}_{\\text{process model}}} \\right ]  {\\underbrace { \\left [\n\\theta_p \\right ] \\left [ \\theta_o \\right ] \\left [ \\sigma_p^2 \\right ]\n\\left [ \\sigma_s^2 \\right ] \\left [ \\sigma_o^2 \\right ]\n}_{\\text{parameter models}}}\n\\end{equation}\n\\]\nExample\nWildebeast population as a function of rainfall\nThe example is informative in that it is completely worked as they\nwill want in the exersizes, so I am going to summarize. First, what is\nthe system and question: wildebeest population in the grasslands of\nTanzania and Kenya appear to be in a steady state caused by\ninstraspecific competition for green plant biomass during the dry\nseeason which in turn is approximately proportional to the annually\nvariable rainfall. That’s a lot to digest, however, the specific\nquestion helps narrow our focus: how does variation in weather modify\nfeedbacks between population density and population growth rate in a\npopulation of wildebeest where precipitation is variable in time. From\nabove, the modeler needs to specify the parts to the model:\n\\[prior \\propto \\text{observation model}\n\\ast \\text{sampling model} \\ast \\text{process model} \\ast\n\\text{parameters model}\\]\nProcess model\nThe question suggests we are looking to model the unobserved true\nnumber of wildebeest in year t. Rather than use \\(z\\), we will use \\(N_t\\) as is customary for population\ncounts. We do have explanatory variables, namely annual rainfall in year\nt, denoted as \\(x_t\\). Hobbs and Hooten\ngive the following for the process model:\n\\[\n\\begin{equation}\n\\tag{6}\nN_t = g(\\mathbf{\\beta},N_{t-1},x_t,\\Delta t) = N_{t-1}e^{(\\beta_0 +\n\\beta_1N_{t-1}+ \\beta_2 x_t + \\beta_3N_{t-1}x_t)\\Delta t}\n\\end{equation}\n\\]\nLooking at the parts, with zero rainfall and wildebeest, \\(e^{\\beta_0\\Delta t}\\) is the base change in\nabundance. The authors spend time discussing what it means to have zero\nrainfall and redefine \\(x_t\\) as the\ndeviation from the long term average rainfall in year t. With this\nreparameterization, \\(\\beta_1\\) is the\nchange in growth rate for each additional animal. The parameter \\(\\beta_2\\) gives the change in population\ngrowth rate due to a deviation from the long-term mean annual growth\nrate. This leaves \\(\\beta_3\\) as the\nmagnitude of rainfall on the population density. Equation (6) is steeped\nin population dynamic theory and is certainly something that would\nrequire some thought.\nEstimating the \\(\\beta\\)’s will\ndefinitely inform the question posed, however, a moments thought will\nlead one to a million parameters left out of the model: predation,\npaching and disease. To add the effects of these unknown and unspecified\nvariables in the model, we add a stochastic component and end at:\n\\[\n\\begin{equation}\n\\tag{7}\n\\left [ N_t \\mid g(\\underline{\\smash{\\beta}}, N_{t-1}, x_t, \\Delta t),\n\\sigma_p^2 \\right ]\n\\end{equation}\n\\]\nSampling model\nThe wildebeest population was estimated using spatially replicated\ncounts of animals on georectified aerial photographs covering some known\narea. A feature of the data is that the pictures did not cover the\nentire area. The data is assumed to represent a statistically\nindependent sample of the population density given as \\(\\frac{N-t}{a}\\) where \\(a\\) is the total area inhabited by the\nanimals.\n\\[\n\\begin{equation}\n\\tag{8}\n\\left [ y_{tj} \\mid \\frac{N_t}{a}, \\sigma_s^2 \\right ]\n\\end{equation}\n\\] Equation (7) gives us our observed density of animals at year\n\\(t\\) on photograph \\(j\\) across the entire habitat. Things\nomitted: bias in estimating animal density on a given picture,\nuncertainty is the counting itself, rainfall is measured without error,\netc which are all dealt with by including a stochastic component, \\(\\sigma_s^2\\).\nObservation model\nIn this case, we are considering our observation of animals to be\nperfect. The authors do provide a hypothetical case for using some sort\nof telemetry as a proxy for counting. They introduce the proxy as a\nprobability for being counted, \\(\\psi\\). Doing this gives the following:\n\\[\n\\begin{equation}\n\\tag{9}\n{\\underbrace {\\left [ y_i \\mid \\psi n_{tj}, \\sigma_o^2 \\right\n]}_{\\text{observation model}}}   {\\underbrace { \\left [ n_{tj} \\mid\n\\frac{N_t}{a}, \\sigma_s^2 \\right ]}_{\\text{sampling model}}}\n\\end{equation}\n\\] The sampling model includes \\(n_{tj}\\) as number of animals that are\ntruly present in picture \\(j\\) at time\n\\(t\\) and the observation model deals\nwith the under count through \\(\\psi\\)\nand associated uncertainty in estimating \\(\\psi\\) by inclusion of \\(\\sigma_o^2\\).\nFull model\nCombining the sampling model (8) with the process model (7) with\nmodels for the parameters, we get to the full model.\n\\[\n\\begin{equation}\n\\tag{9}\n\\left [ {\\underbrace\n{N_t,N_{t-1},\\underline{\\smash{\\beta}},\\sigma_p^2,\\sigma_s^2  }_{\\text{unobserved}}}  \\mid\n{\\underbrace { y_{tj}  }_{\\text{observed}}} \\right ] \\propto\n{\\underbrace {\\left [ y_{tj} \\mid \\frac{N_t}{a}, \\sigma_s^2 \\right\n]}_{\\text{sampling model}}}   {\\underbrace {\\left [ N_t \\mid\ng(\\underline{\\smash{\\beta}}, N_{t-1}, x_t, \\Delta t), \\sigma_p^2 \\right\n]}_{\\text{process model}}} {\\underbrace {\\left [ \\beta_0 \\right ] \\left\n[ \\beta_1 \\right ]\\left [ \\beta_2 \\right ] \\left [ \\beta_3 \\right ]\n\\left [ \\sigma_p^2 \\right ] \\left [ \\sigma_s^2 \\right\n]}_{\\text{parameter models}}}\n\\end{equation}\n\\] I am relatively new to Bayesian networks drawn as acyclic\ngraphs. I like the one they drew as it helps me focus on the different\nrelationships of parameters and quantities observed/calculated. Dashed\narrows are stochastic, solid indicate deterministic relationships with\nquantities observed without error.\n\n\n\nFigure 1: Bayesian network for Wildebeest example from Hobbs\nand Hooten.\n\n\n\nFrom this, we can see the data we need and the various parameters and\nwhere they are involved. If we had included a sampling model, there\nwould have been an additional layer to the figure with the associated\nparameters and dependencies drawn.\n\n\n\n",
    "preview": "books/2022-07-02-hobbs-hooten-ch1/images/HH_fig1.2.1.png",
    "last_modified": "2022-07-03T10:04:17-04:00",
    "input_file": {},
    "preview_width": 544,
    "preview_height": 244
  }
]
